{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI ASSIGNMENT - Handwritten Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import struct\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mpl\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Flatten the Numpy Array from 2D (28*28) to 1D (784*1)\n",
    "'''\n",
    "X_train = X_train.reshape(X_train.shape[0], 784).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 784).astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Change Intensity Values of Pixels from 0 - 255 to 0 - 1\n",
    "'''\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    One Hot Encoding . \n",
    "    i.e.,\n",
    "        0 =  0000000001\n",
    "        1 =  0000000010\n",
    "        2 =  0000000100\n",
    "        3 =  0000001000\n",
    "        .\n",
    "        .\n",
    "        9 =  1000000000\n",
    "'''\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM OPTIMIZER ( with Sigmoid Activation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "'''\n",
    "    This is Input Layer\n",
    "'''\n",
    "model.add(Dense(784 , input_dim=784, kernel_initializer='random_uniform',activation='sigmoid'))\n",
    "\n",
    "'''\n",
    "    These are hidden layers . 16 Neurons in each Layer\n",
    "'''\n",
    "model.add(Dense(16 , activation='sigmoid'))\n",
    "model.add(Dense(16 , activation='sigmoid'))\n",
    "\n",
    "'''\n",
    "    This is output Layer\n",
    "'''\n",
    "model.add(Dense(10 , activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Compile the model\n",
    "'''\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 1.9062 - acc: 0.5161 - val_loss: 1.5188 - val_acc: 0.7819\n",
      "Epoch 2/10\n",
      " - 9s - loss: 1.2046 - acc: 0.8480 - val_loss: 0.9114 - val_acc: 0.9152\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.6958 - acc: 0.9279 - val_loss: 0.5159 - val_acc: 0.9429\n",
      "Epoch 4/10\n",
      " - 9s - loss: 0.4022 - acc: 0.9498 - val_loss: 0.3219 - val_acc: 0.9550\n",
      "Epoch 5/10\n",
      " - 9s - loss: 0.2595 - acc: 0.9623 - val_loss: 0.2255 - val_acc: 0.9637\n",
      "Epoch 6/10\n",
      " - 9s - loss: 0.1855 - acc: 0.9712 - val_loss: 0.1759 - val_acc: 0.9693\n",
      "Epoch 7/10\n",
      " - 10s - loss: 0.1418 - acc: 0.9758 - val_loss: 0.1516 - val_acc: 0.9697\n",
      "Epoch 8/10\n",
      " - 10s - loss: 0.1120 - acc: 0.9807 - val_loss: 0.1270 - val_acc: 0.9738\n",
      "Epoch 9/10\n",
      " - 10s - loss: 0.0903 - acc: 0.9839 - val_loss: 0.1147 - val_acc: 0.9742\n",
      "Epoch 10/10\n",
      " - 9s - loss: 0.0726 - acc: 0.9875 - val_loss: 0.0997 - val_acc: 0.9774\n",
      "Final Baseline Error(Using Sigmoid): 2.26%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Fit the Model\n",
    "'''\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Sigmoid): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM OPTIMIZER (with  RELU ACTIVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 10s - loss: 0.7531 - acc: 0.6974 - val_loss: 0.1663 - val_acc: 0.9525\n",
      "Epoch 2/10\n",
      " - 11s - loss: 0.1421 - acc: 0.9597 - val_loss: 0.1416 - val_acc: 0.9580\n",
      "Epoch 3/10\n",
      " - 9s - loss: 0.0939 - acc: 0.9722 - val_loss: 0.0987 - val_acc: 0.9705\n",
      "Epoch 4/10\n",
      " - 11s - loss: 0.0670 - acc: 0.9803 - val_loss: 0.0891 - val_acc: 0.9744\n",
      "Epoch 5/10\n",
      " - 11s - loss: 0.0508 - acc: 0.9847 - val_loss: 0.0748 - val_acc: 0.9772\n",
      "Epoch 6/10\n",
      " - 10s - loss: 0.0365 - acc: 0.9897 - val_loss: 0.0846 - val_acc: 0.9751\n",
      "Epoch 7/10\n",
      " - 9s - loss: 0.0288 - acc: 0.9915 - val_loss: 0.0714 - val_acc: 0.9806\n",
      "Epoch 8/10\n",
      " - 10s - loss: 0.0198 - acc: 0.9946 - val_loss: 0.0753 - val_acc: 0.9793\n",
      "Epoch 9/10\n",
      " - 10s - loss: 0.0168 - acc: 0.9952 - val_loss: 0.0712 - val_acc: 0.9813\n",
      "Epoch 10/10\n",
      " - 12s - loss: 0.0144 - acc: 0.9958 - val_loss: 0.0772 - val_acc: 0.9800\n",
      "Final Baseline Error(Using Relu): 2.00%\n"
     ]
    }
   ],
   "source": [
    "model_relu = Sequential()\n",
    "\n",
    "model_relu.add(Dense(784 , input_dim=784, kernel_initializer='random_uniform',activation='relu'))\n",
    "\n",
    "model_relu.add(Dense(16 , activation='relu'))\n",
    "model_relu.add(Dense(16 , activation='relu'))\n",
    "\n",
    "model_relu.add(Dense(10 , activation='sigmoid'))\n",
    "\n",
    "model_relu.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boost Optimizer ( with Relu activation ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    Stochastic Gradient Boost Optimiser\n",
    "    lr -- > Learning Rate\n",
    "    momentum --> 0.9\n",
    "'''\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0903 - val_acc: 0.9829\n",
      "Epoch 2/10\n",
      " - 7s - loss: 9.8644e-04 - acc: 0.9998 - val_loss: 0.0914 - val_acc: 0.9822\n",
      "Epoch 3/10\n",
      " - 7s - loss: 6.0617e-04 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 0.9829\n",
      "Epoch 4/10\n",
      " - 7s - loss: 4.9203e-04 - acc: 1.0000 - val_loss: 0.0923 - val_acc: 0.9829\n",
      "Epoch 5/10\n",
      " - 7s - loss: 4.0829e-04 - acc: 1.0000 - val_loss: 0.0922 - val_acc: 0.9828\n",
      "Epoch 6/10\n",
      " - 7s - loss: 3.6390e-04 - acc: 1.0000 - val_loss: 0.0932 - val_acc: 0.9832\n",
      "Epoch 7/10\n",
      " - 7s - loss: 3.2759e-04 - acc: 1.0000 - val_loss: 0.0938 - val_acc: 0.9830\n",
      "Epoch 8/10\n",
      " - 7s - loss: 2.9351e-04 - acc: 1.0000 - val_loss: 0.0946 - val_acc: 0.9833\n",
      "Epoch 9/10\n",
      " - 7s - loss: 2.6945e-04 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 0.9834\n",
      "Epoch 10/10\n",
      " - 10s - loss: 2.5073e-04 - acc: 1.0000 - val_loss: 0.0955 - val_acc: 0.9831\n",
      "Final Baseline Error(Using Relu): 1.69%\n"
     ]
    }
   ],
   "source": [
    "model_relu.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAGRAD OPTIMIZER ( with Relu )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Adagrad Optimizer with Learning Rate 0.01\n",
    "'''\n",
    "agd = optimizers.Adagrad(lr=0.01, epsilon=0.05, decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 2.3650e-04 - acc: 1.0000 - val_loss: 0.0965 - val_acc: 0.9832\n",
      "Epoch 2/10\n",
      " - 8s - loss: 2.1181e-04 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9834\n",
      "Epoch 3/10\n",
      " - 12s - loss: 1.9131e-04 - acc: 1.0000 - val_loss: 0.0974 - val_acc: 0.9832\n",
      "Epoch 4/10\n",
      " - 8s - loss: 1.7537e-04 - acc: 1.0000 - val_loss: 0.0980 - val_acc: 0.9836\n",
      "Epoch 5/10\n",
      " - 8s - loss: 1.6189e-04 - acc: 1.0000 - val_loss: 0.0986 - val_acc: 0.9832\n",
      "Epoch 6/10\n",
      " - 9s - loss: 1.4994e-04 - acc: 1.0000 - val_loss: 0.0988 - val_acc: 0.9836\n",
      "Epoch 7/10\n",
      " - 8s - loss: 1.4138e-04 - acc: 1.0000 - val_loss: 0.0991 - val_acc: 0.9834\n",
      "Epoch 8/10\n",
      " - 8s - loss: 1.3269e-04 - acc: 1.0000 - val_loss: 0.0994 - val_acc: 0.9836\n",
      "Epoch 9/10\n",
      " - 11s - loss: 1.2591e-04 - acc: 1.0000 - val_loss: 0.1000 - val_acc: 0.9832\n",
      "Epoch 10/10\n",
      " - 10s - loss: 1.2002e-04 - acc: 1.0000 - val_loss: 0.1001 - val_acc: 0.9838\n",
      "Final Baseline Error(Using Relu): 1.62%\n"
     ]
    }
   ],
   "source": [
    "model_relu.compile(loss='categorical_crossentropy', optimizer=agd, metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With REGULARIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 47s - loss: 5.9546 - acc: 0.1118 - val_loss: 4.1423 - val_acc: 0.1135\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model_reg = Sequential()\n",
    "\n",
    "\n",
    "model_reg.add(Dense(784 , input_dim=784, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01), kernel_initializer='random_uniform',activation='relu'))\n",
    "\n",
    "model_reg.add(Dense(16 , activation='relu'))\n",
    "model_reg.add(Dense(16 , activation='relu'))\n",
    "\n",
    "\n",
    "model_reg.add(Dense(10 , activation='sigmoid'))\n",
    "\n",
    "agd = optimizers.Adagrad(lr=0.01, epsilon=0.05, decay=0.0)\n",
    "\n",
    "model_reg.compile(loss='categorical_crossentropy', optimizer=agd, metrics=['accuracy'])\n",
    "\n",
    "model_reg.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model_reg.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Sigmoid): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETER TUNING ( Using Relu and Adagrad - Best Combination )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size ( 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 35s - loss: 1.7896e-04 - acc: 1.0000 - val_loss: 0.1021 - val_acc: 0.9840\n",
      "Epoch 2/10\n",
      " - 40s - loss: 8.3799e-05 - acc: 1.0000 - val_loss: 0.1027 - val_acc: 0.9835\n",
      "Epoch 3/10\n",
      " - 34s - loss: 6.1567e-05 - acc: 1.0000 - val_loss: 0.1040 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      " - 35s - loss: 5.2612e-05 - acc: 1.0000 - val_loss: 0.1044 - val_acc: 0.9835\n",
      "Epoch 5/10\n",
      " - 35s - loss: 4.5908e-05 - acc: 1.0000 - val_loss: 0.1049 - val_acc: 0.9837\n",
      "Epoch 6/10\n",
      " - 35s - loss: 4.0758e-05 - acc: 1.0000 - val_loss: 0.1055 - val_acc: 0.9840\n",
      "Epoch 7/10\n",
      " - 35s - loss: 3.6620e-05 - acc: 1.0000 - val_loss: 0.1060 - val_acc: 0.9840\n",
      "Epoch 8/10\n",
      " - 35s - loss: 3.3679e-05 - acc: 1.0000 - val_loss: 0.1060 - val_acc: 0.9840\n",
      "Epoch 9/10\n",
      " - 34s - loss: 3.0730e-05 - acc: 1.0000 - val_loss: 0.1063 - val_acc: 0.9841\n",
      "Epoch 10/10\n",
      " - 35s - loss: 2.8592e-05 - acc: 1.0000 - val_loss: 0.1067 - val_acc: 0.9839\n",
      "Final Baseline Error(Using Relu): 1.61%\n"
     ]
    }
   ],
   "source": [
    "agd = optimizers.Adagrad(lr=0.01, epsilon=0.05, decay=0.0)\n",
    "model_relu.compile(loss='categorical_crossentropy', optimizer=agd, metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=20, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size ( 500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 2.6525e-05 - acc: 1.0000 - val_loss: 0.1068 - val_acc: 0.9840\n",
      "Epoch 2/10\n",
      " - 6s - loss: 2.6303e-05 - acc: 1.0000 - val_loss: 0.1069 - val_acc: 0.9840\n",
      "Epoch 3/10\n",
      " - 6s - loss: 2.6136e-05 - acc: 1.0000 - val_loss: 0.1069 - val_acc: 0.9840\n",
      "Epoch 4/10\n",
      " - 6s - loss: 2.5968e-05 - acc: 1.0000 - val_loss: 0.1070 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      " - 6s - loss: 2.5805e-05 - acc: 1.0000 - val_loss: 0.1071 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      " - 6s - loss: 2.5646e-05 - acc: 1.0000 - val_loss: 0.1071 - val_acc: 0.9840\n",
      "Epoch 7/10\n",
      " - 6s - loss: 2.5485e-05 - acc: 1.0000 - val_loss: 0.1072 - val_acc: 0.9839\n",
      "Epoch 8/10\n",
      " - 6s - loss: 2.5340e-05 - acc: 1.0000 - val_loss: 0.1072 - val_acc: 0.9838\n",
      "Epoch 9/10\n",
      " - 6s - loss: 2.5184e-05 - acc: 1.0000 - val_loss: 0.1073 - val_acc: 0.9837\n",
      "Epoch 10/10\n",
      " - 6s - loss: 2.5039e-05 - acc: 1.0000 - val_loss: 0.1073 - val_acc: 0.9841\n",
      "Final Baseline Error(Using Relu): 1.59%\n"
     ]
    }
   ],
   "source": [
    "agd = optimizers.Adagrad(lr=0.01, epsilon=0.05, decay=0.0)\n",
    "model_relu.compile(loss='categorical_crossentropy', optimizer=agd, metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=500, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size ( 1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 2.4871e-05 - acc: 1.0000 - val_loss: 0.1073 - val_acc: 0.9840\n",
      "Epoch 2/10\n",
      " - 5s - loss: 2.4797e-05 - acc: 1.0000 - val_loss: 0.1074 - val_acc: 0.9838\n",
      "Epoch 3/10\n",
      " - 5s - loss: 2.4718e-05 - acc: 1.0000 - val_loss: 0.1074 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      " - 5s - loss: 2.4643e-05 - acc: 1.0000 - val_loss: 0.1074 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      " - 5s - loss: 2.4573e-05 - acc: 1.0000 - val_loss: 0.1074 - val_acc: 0.9839\n",
      "Epoch 6/10\n",
      " - 5s - loss: 2.4503e-05 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      " - 5s - loss: 2.4431e-05 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9838\n",
      "Epoch 8/10\n",
      " - 5s - loss: 2.4357e-05 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9839\n",
      "Epoch 9/10\n",
      " - 5s - loss: 2.4287e-05 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9839\n",
      "Epoch 10/10\n",
      " - 5s - loss: 2.4222e-05 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9839\n",
      "Final Baseline Error(Using Relu): 1.61%\n"
     ]
    }
   ],
   "source": [
    "agd = optimizers.Adagrad(lr=0.01, epsilon=0.05, decay=0.0)\n",
    "model_relu.compile(loss='categorical_crossentropy', optimizer=agd, metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=1000, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size ( 10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 7s - loss: 2.4122e-05 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9839\n",
      "Epoch 2/10\n",
      " - 5s - loss: 2.4114e-05 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9839\n",
      "Epoch 3/10\n",
      " - 5s - loss: 2.4107e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      " - 5s - loss: 2.4100e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      " - 5s - loss: 2.4093e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9839\n",
      "Epoch 6/10\n",
      " - 5s - loss: 2.4087e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      " - 5s - loss: 2.4080e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9839\n",
      "Epoch 8/10\n",
      " - 5s - loss: 2.4075e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9839\n",
      "Epoch 9/10\n",
      " - 5s - loss: 2.4067e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9839\n",
      "Epoch 10/10\n",
      " - 5s - loss: 2.4059e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9839\n",
      "Final Baseline Error(Using Relu): 1.61%\n"
     ]
    }
   ],
   "source": [
    "agd = optimizers.Adagrad(lr=0.01, epsilon=0.05, decay=0.0)\n",
    "model_relu.compile(loss='categorical_crossentropy', optimizer=agd, metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=10000, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate (0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 2.4357e-05 - acc: 1.0000 - val_loss: 0.1077 - val_acc: 0.9837\n",
      "Epoch 2/10\n",
      " - 7s - loss: 2.3515e-05 - acc: 1.0000 - val_loss: 0.1079 - val_acc: 0.9835\n",
      "Epoch 3/10\n",
      " - 6s - loss: 2.3006e-05 - acc: 1.0000 - val_loss: 0.1082 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      " - 6s - loss: 2.2283e-05 - acc: 1.0000 - val_loss: 0.1084 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      " - 7s - loss: 2.1715e-05 - acc: 1.0000 - val_loss: 0.1086 - val_acc: 0.9838\n",
      "Epoch 6/10\n",
      " - 6s - loss: 2.1251e-05 - acc: 1.0000 - val_loss: 0.1089 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      " - 6s - loss: 2.0711e-05 - acc: 1.0000 - val_loss: 0.1091 - val_acc: 0.9839\n",
      "Epoch 8/10\n",
      " - 6s - loss: 2.0171e-05 - acc: 1.0000 - val_loss: 0.1093 - val_acc: 0.9840\n",
      "Epoch 9/10\n",
      " - 6s - loss: 1.9712e-05 - acc: 1.0000 - val_loss: 0.1094 - val_acc: 0.9840\n",
      "Epoch 10/10\n",
      " - 6s - loss: 1.9227e-05 - acc: 1.0000 - val_loss: 0.1097 - val_acc: 0.9839\n",
      "Final Baseline Error(Using Relu): 1.61%\n"
     ]
    }
   ],
   "source": [
    "agd = optimizers.Adagrad(lr=0.05, epsilon=0.05, decay=0.0)\n",
    "model_relu.compile(loss='categorical_crossentropy', optimizer=agd, metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=500, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate (0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 7s - loss: 1.9010e-05 - acc: 1.0000 - val_loss: 0.1099 - val_acc: 0.9837\n",
      "Epoch 2/10\n",
      " - 6s - loss: 1.8095e-05 - acc: 1.0000 - val_loss: 0.1101 - val_acc: 0.9837\n",
      "Epoch 3/10\n",
      " - 6s - loss: 1.7348e-05 - acc: 1.0000 - val_loss: 0.1106 - val_acc: 0.9837\n",
      "Epoch 4/10\n",
      " - 6s - loss: 1.6597e-05 - acc: 1.0000 - val_loss: 0.1109 - val_acc: 0.9838\n",
      "Epoch 5/10\n",
      " - 7s - loss: 1.5880e-05 - acc: 1.0000 - val_loss: 0.1112 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      " - 7s - loss: 1.5418e-05 - acc: 1.0000 - val_loss: 0.1116 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      " - 6s - loss: 1.4778e-05 - acc: 1.0000 - val_loss: 0.1119 - val_acc: 0.9841\n",
      "Epoch 8/10\n",
      " - 6s - loss: 1.4318e-05 - acc: 1.0000 - val_loss: 0.1121 - val_acc: 0.9842\n",
      "Epoch 9/10\n",
      " - 6s - loss: 1.3778e-05 - acc: 1.0000 - val_loss: 0.1123 - val_acc: 0.9838\n",
      "Epoch 10/10\n",
      " - 6s - loss: 1.3398e-05 - acc: 1.0000 - val_loss: 0.1124 - val_acc: 0.9837\n",
      "Final Baseline Error(Using Relu): 1.63%\n"
     ]
    }
   ],
   "source": [
    "agd = optimizers.Adagrad(lr=0.1, epsilon=0.05, decay=0.0)\n",
    "model_relu.compile(loss='categorical_crossentropy', optimizer=agd, metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=500, verbose=2)\n",
    "scores = model_relu.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Final Baseline Error(Using Relu): %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
